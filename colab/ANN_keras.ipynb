{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwAMIG3ik8JTcg1GzgLtsb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cateto/python4NLP/blob/main/colab/ANN_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWflhbJxC9Q-"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential() # 층을 추가하기 위해 순차 모델임을 추가\n",
        "\n",
        "model.add(Dense(8, input_dim=4, kernel_initializer='uniform', activation='relu')) \n",
        "# init 대신 kernel_initializer의 이름으로 대입해서 사용 -> 초기값 설정기를 uniform(균일 분포)로 설정 cf.normal(가우시안 분포)\n",
        "# 참고 : https://sevillabk.github.io/Dense/\n",
        "\n",
        "# 해석 : 입력층(4)과 다음 은닉층(8) 그리고 은닉층의 활성화 함수는 relu\n",
        "\n",
        "model.add(Dense(8, activation='relu')) #은닉층(8)의 활성화 함수는 relu \n",
        "model.add(Dense(3, activation='softmax')) #출력층(3)의 활성화 함수는 softmax 출력층이니까...!\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfBUR1jKEAhB"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, kernel_initializer='uniform'))\n",
        "model.add(layers.Activation('Softmax'))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkeqwuGCYkyT"
      },
      "source": [
        "케라스의 옵티마이저 ( 최적화러...최적화도구,,,)\n",
        "=> 실제값과 예측값의 오차로부터 옵티마이저를 통해 가중치를 업데이트\n",
        "\n",
        "https://keras.io/ko/optimizers/#adam\n",
        "\n",
        "- 배치 경사 하강법 : 전체 데이터가 하나의 batch이므로 한 epoch당 시간이 오래걸림\n",
        "- 확률적 경사 하강법(SGD) : batch_size가 1이므로 하나의 데이터를 선택하여 계산, 시간은 상대적으로 빠름 (정확도, 변경 폭은 불안정)\n",
        "- 미니 배치 경사 하강법(가장 많이 사용) : batch_size를 임의의 값으로 정함\n",
        "- 모멘텀(Momentum) : 경사하강법에서 계산된 접선의 기울기 한 step 전의 기울기를 일정한 비율로 반영하여 로컬 미니멈에 도달했을 때 탈출할 수 있도록 함. \n",
        "- 아다그라드(Adagrad) : 모든 매개변수에 다른 학습률을 적용시킴\n",
        "- RMSprop : 아다그라드의 단점 개선, 계속 학습하다보면 학습률이 떨어지지 않도록 방지\n",
        "- Adam : 알엠에스프롭과 모멘텀을 합침, 방향과 학습률을 모두 고려 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhma03cyLvAS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
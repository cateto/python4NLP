#### RNN Encoder-Decoder

문장이 길어질수록 번역성능이 떨어진다.

-> 장기의존성 문제, 보틀넥발생(병목현상)



#### Attention Function

Attention(Q, K, V) = Attention Value

Q-> K 유사도 * V = V

V1 + V2 + V3 = Attention Value



#### Attention 메커니즘으 ㅣ종류

다른점! Q, K의 유사도 구하는 attention score function이 다름.

예전엔 align이라고 부르기도 했음.

alignment function이라고 했었음.



#### Dot product Attention = 루옹 어텐션

Q : 디코더

K, V : 인코더

디코더의 현재시점 hiddenstate(Query) * 인코더의 전체시점 hiddenstate(Key)

-----  소프트 맥수 통과! ---------

softmax함수를 통과시키는 이유 : 총 합이 1이 되도록...

value * 각각

==> 다 더함

= attention value

출력층으로 

and 

신경망 한번 지남 = 가중치 곱함

st틸다

q1. 출력층에서 가중치 곱해서 나온 값이 번역 TASK랑 어떻게 연결되는 건가요?

softmax를 통과하면 예측해야하는 단어를 고른다. 

중간에 attention이 출력층에 들어갔다는 것이 다른 점임.



#### concat attention = 바다나우 어텐션

번역된 문장의 순서가 바뀌어도 어텐션 스코어에서 잘 찾아낸다.

루옹어텐션과 다르게 바로 이전 시점의 hiddenstate로 함.



q2. context vector란?

입력문장을 전체를 훑고 나서 보낸 값

기존 번역은 

q3. 인코더 문장을 다시보는 것이 Seq2Seq 한계인 벡터차원이 고정되어있는 부분이 어떻게 보완이되는지 궁금합니다

q4. 어텐션을 쓰면 rnn, bi-rnn 사용하지 않아도 되는건가요?

우리는 어텐션을 사용하면 성능을 올릴 수 있을 것이라고 예상했는데

결과는 생각보다 차이가 더 컸는데, 어텐션을 안쓰면 서비스에 쓰지도 못할 정도였다.

= 어텐션을 무조건 써야 결과가 제대로 나온다.

= 인코더는 biLSTM 2층을 썼고, 디코더는 LSTM 2층을 썼다.

= 디코더에는 BiLSTM을 쓸 수가 없습니다. (구조적으로 쓸수가 없음)

= LSTM이 계속 우세했다.

= 2, 3층으로 하면 성능이 소폭올라가는데 큰 차이는 없었으며  3층에서 4,5층으로 높이는 것은 무의미한 수준이었다.



q5. 어텐션이 앞에서 중요한 단어에 주목하는것이라고 했는데 어떤 원리로 그렇게 되는건가요~?



#### Transformer

self-attention 

=Q, K, V 가 같을때



q6. 셀프어텐션이 6층 구조 유지랑 어떤 연관이 있는가?



실제로 행렬 연산으로 이루어짐.

병렬 연산 후

전치해줌 (열, 행 바꿈)



self attention 입력 값 512인데 64로 결과가 도출되는경우

방금 한 것을 8번 수행하는것임!

= multi-head attention



q7. softmax를 행방향으로 적용해야하는 이유가 있나요?? 열방향으로 해도 어차피 단어 하나 하나에 대해 적용되는거 아닌가요??



q8. 멀티헤드어텐션 8개의 결과가 다른 이유를 가중치가 달라서(?)라고 이해해도 되나요..?!

가중치가 달라서라고 이해해도 됨.



q9. 그런데 RNN처럼 순차적으로 주지않고 한번에 주는 장점이 무엇인가요?

가장 



q10. 그럼 트랜스포머는 정확도 향상보다는 계산속도 향상에 장점이 있다고 봐도 되나요?

6층이나 되기때문에 속도 향상,,음~~ 그래도 다른 것보단 성능에서 유리함.



q11. 방금 해당 가중치 행렬 8개가 점차 update 되면서 8개가 되는 건가요 아니면 처음부터 8개의 초기화된 가중치 행렬을 부여하는 건 가요

처음 시작할때부터 8개임.



q12. 모델복잡도가 높아보이는데 그러면 train 데이터의 수가 적으면 불리한가요?

모델이 크기때문에 train 데이터 적을때 쓰는 것은 아님.

그래도 왜 쓰냐면~!!

기계번역 자체가 애초에 train data가 양이 많음.

수백만개가 있어야 돌아가는 것임.










#### 1주차 복습 -> 오늘과 연관

#### Language Model => 단어 시퀀스에 확률을 할당하는 모델 = 더 그럴듯한 문장을 선택할 수 있음

구글 검색 완성 => 확률을 계산해서 확률이 가장 높은 문장을 보여주는 것

딥러닝을 모르는데 구글 검색 완성을 만들 수 있는  법 => db에서 기록 순으로



#### 예전 랭귀지 모델 = 각 단어의 확률을 전부 곱해서 = (p(w5|w1,w2,w3,w4))

=> 카운트 기반 계산  시퀀스 통계(갯수) / 시퀀스 통계(갯수)



#### 한계 !!!! 방대한 집합체인 구글 검색엔진에서 조차 간단한 문장이 포함된 결과가 나오지 않음!!!!

간단한 문장도 등장하지 않을 수 있는 확률



#### 대안 = N-gram Language Model

N-gram  = N 개의 연속적인 단어 나열 (단어들의 n 개 묶음)

n = 1 Unigram

n = 2 Bigram

n = 3 Trigram

n = 4 4gram

I am a student => bigram을 전부구해라!

(I, am), (am, a), (a, student)

n-1개만 확률 계산에 포함! 

P(Wn|~~~~Wn-1)

=> 카운트가 안되는 경우가 훨씬 줄어듦

but n 이 5-6정도 되면 추정하기 어려워짐

작아도 문제가됨 = > why??? 장기 의존성 문제 생김



Q1. 장기의존성 문제가 무엇인가요?

앞의 단어가 중요한 단어인데 문장이 길어졌을 때 앞의 단어를 고려하지 못하는 경우.

data = '내가 모스크바에 갔는데 거기서 친구를 만났는데 프랑스 친구랑만나서 한인 식당에 가서 떡볶이를 먹었어'

question = '그렇다면 나의 위치는 어디야?'

발생하는 이유 : ngram은 그 이전의 단어를 무시함. 

#### Sparsity Problem

#### 인공싱경망은 보지 않은 시퀀스도 예측 가능. 

워드임베딩 = 단어를 벡터로 만드는 방법 and 단어 벡터간 유사도를 얻을 수 있음

=> 이로부터 훈련데이터에 없던 시퀀스라도 생성이 가능함.



#### NNLM(신경망 언어 모델) 도 곧 n개의 이전 단어로부터 n+1 단어를 예측하는 모델임.

input은 원핫 벡터

(n번째가 1인)원핫벡터와 가중치 행렬의 곱은 결과적으로는 가중치 행렬의 n번째 위치의 행을 꺼내오는 것과 같음

(=lookup table)



Q2. 중간에 가중치는 어떻게 계산되는 것인가요?

행렬곱으로 계산함!

가중치는 처음에 랜덤 초기화!

훈련시키면 바뀜!



Q3. linear와 non-linear

 linear => 활성화 함수가 없음

non-linear => 활성화 함수가 있음

sigmoid, softmax, relu(은닉층에서 많이 사용)



Q4. 중간에 히든레이어가 있어야하나요?

2008년 당시에는 딥러닝이니까 넣었음 -> 뒤에서 나오는 모델은 없어도됨.



Q5. projection layer의 차원이 20이 된다는 것은 axis 1 기준으로 붙이게 되는 건가요?

concatenate => 이어붙임



Q6. 히든레이어 갯수에 따른 아웃풋 차이가 어떻게 되나요?

히든레이어 갯수 늘어남 = 가중치 행렬의 갯수가 늘어남

= 모델의 용량이 커진다



Q7. 가중치 행렬

가중치 행렬 = 벡터가 우리가 원하는 최종 결과물!!!!!!

weight matrix가 전부 학습이 되면 input에 따라 사람이 봤을때도 자연스러운(?)에 가까운 output을 내게됨!!



#### 워드임베딩

단어를 벡터화 한다.

원핫 벡터 = 단어 사이즈 (고차원) / 희소벡터(대부분 값이 0) / 수동적으로 표현

임베딩 벡터 = size를 정할 수 있음 (저차원) / 밀집벡터(대부분 값이 실수) / 훈련데이터로 부터 학습

- 랜덤 초기화 임베딩(오차를 구하는 과정에서 학습, Task에 맞도록 최적화~!! )

- 사전 훈련 임베딩 (이미 학습되어있음. 대표적 알고리즘 (Word2Vec, FastText, GloVe) = 단어의 일반화된 의미에 맞게 최적화  

#### 랜덤초기화 방법 : 행(vocab_size) / 렬(size 정하면 됨..일반저그로 vocab_size보단 한참 작아야함 512이하~)

Q8. 그럼 NNLM논문에서는 embedding layer는 파라미터로 w만 가지고있고 hidden layer랑 softmax 적용한 layer는 w와 b 모두 가지고 있는 것이 맞나요~??

임베딩테이블(프로젝션테이블)은 편향이없음!!!



#### Pretrained Word Embedding (이미 훈련된걸 가져와 씀!)

task에 따른 성능은 랜덤초기화 방법이 더 좋은 것이 맞음!

그럼에도 써야하는 경우가 종종 있음! = 풀고자하는 문제의 학습데이터가 적을 경우.

적어도 단어의 일반화된 의미를 알고 있으므로~~~ 데이터가 적으면 훈련된 걸 가져와 쓰는게 좋다.



#### Word2Vec : NNLM을 개선한 모델 

#### / 이전 단어로 다음 단어 예측 하는거 말고 양쪽 문맥을 다 고려할수있도록! CBoW 

주변단어

concatenate의 평균으로 정보 전달!!

hidden layer 날려버림

#### / 중심 단어로 주변 단어를 예측! Skip-gram

#### / Skipgram with negative Sampling 주변단어 /중심단어 넣으면 이진분류 yes or no

=> 가정 : 비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다.

=>  위키독스 링크에서 실습해보기!!!!!!!!!!!!!!!!!!!!! ~~~~~

#### 벡터의 내적 : 벡터의 유사도를 구하는 방법. 두 벡터의 내적값이 크다는 건 두 백터가 유사하다.



Q8. pre-trained 사용시 내 데이터에 대응하는 미리 훈련된 벡터가 없으면 어떻게 하나요?

UNK (OOV)로 대체



Q9. Skip-gram의  input은 1개인데 output이 4개가 나오는 원리가 어떻게 되나요?

데이터를 4개로 구성한 것. input 1 -> input 1



Q10. pretrained 임베딩 사용해서 task에 맞춰 학습시킬 때 임베딩테이블은 weight 업데이트를 하지는 않는건가요?

선택지 2개 -> 추가학습 or 그냥 기존 훈련된걸로 학습

추가학습한다고 성능이 올라간다는 보장이 되지 않음.

Trainable=False(추가핛흡x)

Trainable=False(추가핛흡o)



Q11. window 사이즈를 어떻게 책정해야하나?

작으면 -> 높은 유사도 but 반의어도 포함될 수 잇음

크면 -> 관련있는 단어들을 군집하는 효과를 가짐



#### Word Embedding의 한계

똑같이 생긴 단어는 똑같은 정수로 매핑이됨..

문맥을 고려한 의미를 담고 있지 않음

2018년까지 워드임베딩.. 그 이상으로 contextual Embedding 등장으로 성능이 높아짐! (문맥 고려)



과제 있음..!!





















22p. 
가중치행렬을 왼쪽에 둘 수도 있지만 곱하는 순서가 다를뿐 같음.

행렬 연산을 이해해야 RNN 이해가 용이함.

#### RNN
연속적인 시퀀스 (뭔가가 나열되어있는것 , 문장이든... 텍스트가 아닐 경우 = 주가 데이터, 시간에 따라 값이 달라지는 것들)
#### '시간 개념이 있는 데이터!!!!!'

노드가 과거의 자기자신의 입력 + 새로운 입력을 가지고 연산을 함.
입력의 길이만큼 신경망이 펼쳐진다.

q1. y1, y2, y3 각 시점의 출력에서 yt만 의미가 있느냐?

어떤 문제를 푸느냐에 따라 다른데, 텍스트 분류의 경우에는 yt만 의미가 있다.
모든 시점의 y가 의미있는것 뒤에서 보여줌.
(27p.)

ex) 햄버거 먹을때 몸무게가 늘어나겠지만 자신이 바뀐거 아님.. 과가ㅓ의 자기 자신에게영향을 받고 있지만 다른 사람이 된것은 아님


q2.  개체명 인식은 다중클래스 분류인가요?
순간순간마다 다중 클래스 분류를 하는 것이다.
다만 다른 점은 rnn이기 때문에 앞의 입력에 대한 가중치를 반영한 것이다.

q3. 개체명 인식은 각 단어별로만 분류해도 될 것 같은데 이전 시점의 정보가 왜 필요한 것인가요? 문맥을 고려하기 위함인가요? 

ex) 대통령 문재인이 A라는 장소에 갔다.
대통령 오바마가 A라는 장소에 갔다.

cf. 오바마라는 음식점이 사전에 등록되어 있었다?!!

(대통령 - 오바마) => 문맥을 고려하면 사람이름
(오바마) => 단순히 하나만 고려하면 장소이름

품사 태깅의 경우에도 문맥을 보고 맞춰야 함.

*은닉층 계산*

시그모이드는 출력층
하이퍼볼릭탄젠트함수는 은닉층

q4. 33p -> 34p에서 히든스테이트(ht-1)와 xt의 크기가 다를 수 잇는지?
시점마다 가중치 갯수가 바뀔수는 없다.


#### 36p 은닉층만 표현한것 vs 출력층까지 표현한 것 =>그림의 차이이지 같은것임.

#### RNN의 고질적 문제 = 장기의존성 문제

2쪽까지 읽으면 기억남
300쪽 읽는 시점에서 2쪽 내용을 기억? 어려움..
만약에 핵심내용이 2쪽에 있었다면? 성능이 낮아지는것임.


q5. 하이퍼 탄젠트와 활성화 함수 부분 한 번 더 설명해주시면 감사하겠습니다
은닉층에선 주로 ReLU (가장 많이 쓰임 MLP에서 쓰지유... 가운데 층에서!)
Tanh (ReLU보다는 안쓰이지만 RNN에서는 Tanh을 쓰는 것이 적합)
why? 양수가들어오면 값을 그대로 보존하므로 과거값 계속 곱할 텐데 ReLU쓰면 값이 계속 커짐!!!

q6. 그러면 입력데이터에서
가령 하나의 문장(전체 time stamp 합친)이 하나의 샘플이 되는건가요
아니면 각 벡터화된 단어(time stamp)가 하나의 샘플이 되는건가요?

ex) 네이버 영화 리뷰 감성 분류
아 이 영화 개 노잼 : 0
영화 재밌네요 : 1
RNN에서는 아 / 이 / 영화 / 개노잼 (순차적으로 하나씩 입력받고) => 결과!

q6-1. 하나의 문장이 하나의 샘플이면
샘플마다 time stamp 크기는 모두 같아야하나요??

패딩해주면 문장 길이가 똑같아지기 때문에 상관없음.

ex) 아 이 영화 개 노잼 : 0
영화 재밌네요 0 0 : 1

q7. 개체명인식같은 매 시점마다 출력하는 task에는 장기의존성 문제에서 벗어날 수 있나요?

RNN은 장기의존성 문제가 본질적으로 있는 것임.
개체명 인식도 어차피 이전 가중치에 영향을 받기때문에 어쩔 수 없음.
문장이 길어질 수록 성능이 떨어질 수 밖에 없음.

q8. RNN에서 Wh도 똑같은 값을 쓰는 건가여? h1->h2->h3 .. 로 갈떄요

p.33 
Wh는 Wx 존재 자체는 시점에 따라 달라지는 게아니라 같음
다만, 처음에는 랜덤값이고 학습에 따라 바뀜.


q9. 단어가 어떻게 들어가느냐?
I like apple -> 57, 3, 7
Embedding(단어 집합의 크기, 벡터 차원)
=> 단어 집합의 크기를 가지는 행
=> 벡터 차원의 크기를 가지는 열

ex) 이중 분류
model = Sequential()
model.add(Embedding(5000, 128))
model.add(SimpleRNN(256)) -> 은닉층 크기를 정해준것
model.add(Dense(1, activation='sigmoid')) -> 출력층

ex) 다중 분류
model = Sequential()
model.add(Embedding(5000, 128))
model.add(SimpleRNN(256)) -> 은닉층 크기를 정해준것
model.add(Dense(3, activation='softmax')) -> 출력층


#### vanlia RNN = simple RNN (가장 간단한 RNN)

#### LSTM (장기의존성 문제를 개선)
수식을 외울 필요는 없지만 ㅋㅋ
달라진점은 알아야함!
- RNN에 없었던 Cell state가 생김!
- 전달값이 하나 더 생김
- 입력 게이트 
- 삭제 게이트
- cell state를 계산할때 입력게이트 ,삭제게이트를 씀.
- 만약 삭제게이트 0이면 입력게이트만 열린 상태
- 만약 입력게이트 0이면 삭제게이트만 열린 상태
- 입력게이트 / 삭제게이트의 의미
- 입력 게이트 : 현재 시점의 의미를 얼마나 반영할지를 결정
- 삭제 게이트 : 이전 시점의 의미를 얼마나 반영할지를 결정
- 학습을 통해 조절하는 것임

q10. 입력게이트가 0 , 삭제게이트가 0이되는것은 극단적인 예시인가요? 혹은 학습을 통해 0이 되는 일이 잦나요?

최고의 성능을 높이는 방향으로 학습되기 때문에 일반적으로는 둘다 적당한 값을 찾아냄.

#### GRU (장기의존성 문제를 개선)
- LSTM에 비해 가중치와 편향 개수가 줄었음
- 데이터가 많을때는 LSTM보다 GRU가 잘돌아감.

#### RNN을 RNN에 올릴 때 주의 할 점
- 아래에 있는 RNN을 전부 위로 보내줘야 함.
- 데이터가 많을 때 모델의 크기를 키워야 성능이 좋음.

#### 양방향 rnn (bidirectional rnn)
- 결정할때마다 양쪽 문맥을 다 보겠다!
- 문맥이라는 것은 양쪽에 다 있음!
- concatenate를 함!!! (4차원 + 4차원 = 8차원)

#### Deep rnn
- bidirectional을 한번 더 쌓으면 Deep rnn

q11. bidirectional RNN은 backpropagation이 어떤식으로 되나요..?

학습을하면 각각 둘다 영향 받도록 역전파(back propagation)가 됨.

q12. 분야별로 샤용하기 좋은 task를 추천해주실 수 있나요?

many to many 문제를 풀떄는 bidirectional RNN이 적합.
구글 번역기 문제에서 Deep RNN(양방향 2층짜리)를 썼다는 예시가 있음.
-> 원 글을 깊이 이해하고 번역해야하기 때문에.. 3-4층 올리는 것은 의미가 없었음. (3층까지 잘 안쌓음)
깊이 이해하려면 양방향이 맞긴하지만 층을 쌓는 것은 데이터 양에 따라 판단.

양방향을 쓸 수 없는 경우 뒤에서 설명!!!

q13. 그러면 해당 PPT에서 첫번째 Deep RNN과 Bidirectional RNN에서는 
Wh(은닉층 weight파라미터)가 2개 생성되는게 맞나요~??

yes


딥러닝은 원래 병렬연산을 하기때문에
3d 텐서를 입력받음 (문장 갯수, timesteps(단어 갯수), input_dim)

q14. 이해가 잘안되서 ㅠ task를 예로 들어주실 수 있나요?
many to many : return_sequences=True
many to one : return_sequences=False, return_state=False


LSTM 같은 경우에는 return_state=True의 경우
셀스테이트 ,히든스테이트 반환.

실행할때마다 가중치가 random초기화떄문에 바뀜!!!!
4강.ipynb에서 가중치를 고정해놓으심!!!

양방향으로 many to one할때는
return_sequences=False, return_state=True
https://wikidocs.net/images/page/94748/bilstm3.PNG
정방향 마지막 + 역방향 마지막

many to many
return_sequences=True, return_state=True
https://wikidocs.net/images/page/94748/bilstm1.PNG

#### 양방향을 쓸 수 없는 경우 뒤에서 설명!!!
다음단어 예측 하는 모델은 양방향과 모순됨.

q15. 커널에 들어가는 가중치는 어떻게 정해지는 것인가요?
랜덤 초기화 되어있고 학습되는 과정에서 오차를 줄이는 방식으로 학습됨.

75p. max-pooling 큰 값만 가져가므로 가중치 필요 x

82p. 자연어처리에서는 1방향으로 가기떄문에 1d convolution

q16. 1D covolution도 문맥을 고려했다고 할 수 있나요?

이미지 처리 -> 부분 공간을 고려하는 것이 핵심!

문맥을 반영안한다고 볼 수 없지만 -> 부분 공간의 특징을 잡아낸 다는 것이 특징! (=단어를 몇개의 묶음으로 보고 판단할 것인가 , n gram과 유사!)

q17. 커널사이즈는 사용자가 정할 수 있나?

https://wikidocs.net/80787

model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))
커널사이즈 5짜리를 32개쓰겠다.
model.add(GlobalMaxPooling1D())

https://wikidocs.net/85337

for sz in [3, 4, 5]:
    conv = Conv1D(filters = num_filters,
                         kernel_size = sz,
                         padding = "valid",
                         activation = "relu",
                         strides = 1)(z)
    conv = GlobalMaxPooling1D()(conv)
    conv = Flatten()(conv)
    conv_blocks.append(conv)

3짜리 128개
4짜리 128개
5짜리 128개 쓰겠다.

뭐가 최적인지는 정답이 없다.



과제 있음 ㅠ! 제발하자~~~~~~



#### ELMo

문맥에 따라 단어 벡터값이 결정됨.



텍스트데이터를 엄청 모음~~~~~~

-> RNN으로 엄청 모은 학습데이터를 학습시킴 (레이블링 필요없음)

-> 순방향, 역방향으로 RNN언어모델을 따로 학습시킴

-> 각각 뽑아옴 ht를

-> 순방향의 경우 ht는 이전 단어가 반영이 된것임.

-> 역방향의 경우 ht는 이후 단어가 반영이 된것임.

-> concatenate함 * 가중치를 각각 줌(처음에는 랜덤초기화 됨)

-> 다 더하면 vector 1개가 나옴 (가중 합)


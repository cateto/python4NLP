{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa7mST7S7eIblT3kIHHjCU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cateto/python4NLP/blob/main/kobart/kobart_summarization_to_torchscript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AugDp2UZP_T",
        "outputId": "192d4bc1-7cca-43b8-c266-6186c0cc0aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 29.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPoVJO7ZIVH",
        "outputId": "4b9ebf46-6438-4b1c-b71d-0bec53d9d000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.onnx import export\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
        "\n",
        "sentence_text = '요약을 위해 작성하는 글입니다. 사실 이 글의 요점은 아무 주제도 없다는 것에 있습니다. 부디 기계가 이 요점을 잘 정리해서 보여주었으면 좋겠습니다.'\n",
        "max_length = 512\n",
        "\n",
        "model_input = tokenizer(\n",
        "            sentence_text,\n",
        "            return_tensors='np',\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "input_ids = model_input[\"input_ids\"]\n",
        "attention_mask = model_input[\"attention_mask\"]\n",
        "\n",
        "tokens_tensor = torch.tensor(input_ids)\n",
        "segments_tensors = torch.tensor(attention_mask)\n",
        "\n",
        "class Pytorch_to_TorchScript(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pytorch_to_TorchScript, self).__init__()\n",
        "    self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization').cuda()\n",
        "  def forward(self, data, attention_mask=None):\n",
        "    return self.model.generate(data.cuda(), num_beams=4,  max_length=512,  eos_token_id=1)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating the trace\n",
        "pt_model = Pytorch_to_TorchScript().eval()\n",
        "traced_model = torch.jit.trace(pt_model, (tokens_tensor, segments_tensors))\n",
        "torch.jit.save(traced_model, \"traced_bart.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1RGSJBQkRr2",
        "outputId": "20f1b81a-f5d1-497d-f08d-d91a6d336220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1412: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input_ids_seq_length >= max_length:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:183: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2758: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if num_beams * batch_size != batch_beam_size:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:915: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input_shape[-1] > 1:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/logits_process.py:641: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if cur_len == self.max_length - 1:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:220: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if not (batch_size == (input_ids.shape[0] // self.group_size)):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:238: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self._done[batch_idx]:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:252: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:256: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (eos_token_id is not None) and (next_token.item() == eos_token_id):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:290: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:291: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  next_scores[batch_idx].max().item(), cur_len\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2880: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/stopping_criteria.py:113: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return any(criteria(input_ids, scores) for criteria in self)\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:269: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  next_score.item(),\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:882: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  self.worst_score = min(score, self.worst_score)\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:875: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if len(self) < self.num_beams or score > self.worst_score:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:878: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:317: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self._done[batch_idx]:\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:337: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:343: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:354: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sent_lengths_max = sent_lengths.max().item() + 1\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:364: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sent_lengths.min().item() != sent_lengths.max().item():\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:378: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sent_lengths[i] < sent_max_len:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMZXpdqPZ1Y6",
        "outputId": "c8dcbbea-c516-4d3a-be75-e1b382366f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  traced_bart.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = torch.jit.load(\"traced_bart.pt\")\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj9pIwiiZWhz",
        "outputId": "ea40d5d0-c529-4744-f94e-ffe8e27badeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecursiveScriptModule(\n",
              "  original_name=Pytorch_to_TorchScript\n",
              "  (model): RecursiveScriptModule(\n",
              "    original_name=BartForConditionalGeneration\n",
              "    (model): RecursiveScriptModule(\n",
              "      original_name=BartModel\n",
              "      (shared): RecursiveScriptModule(original_name=Embedding)\n",
              "      (encoder): RecursiveScriptModule(\n",
              "        original_name=BartEncoder\n",
              "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
              "        (embed_positions): RecursiveScriptModule(original_name=BartLearnedPositionalEmbedding)\n",
              "        (layers): RecursiveScriptModule(\n",
              "          original_name=ModuleList\n",
              "          (0): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (1): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (2): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (3): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (4): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (5): RecursiveScriptModule(\n",
              "            original_name=BartEncoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "        )\n",
              "        (layernorm_embedding): RecursiveScriptModule(original_name=LayerNorm)\n",
              "      )\n",
              "      (decoder): RecursiveScriptModule(\n",
              "        original_name=BartDecoder\n",
              "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
              "        (embed_positions): RecursiveScriptModule(original_name=BartLearnedPositionalEmbedding)\n",
              "        (layers): RecursiveScriptModule(\n",
              "          original_name=ModuleList\n",
              "          (0): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (1): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (2): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (3): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (4): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "          (5): RecursiveScriptModule(\n",
              "            original_name=BartDecoderLayer\n",
              "            (self_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (activation_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
              "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (encoder_attn): RecursiveScriptModule(\n",
              "              original_name=BartAttention\n",
              "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
              "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
              "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
              "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
              "          )\n",
              "        )\n",
              "        (layernorm_embedding): RecursiveScriptModule(original_name=LayerNorm)\n",
              "      )\n",
              "    )\n",
              "    (lm_head): RecursiveScriptModule(original_name=Linear)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "sentence_text = \"\"\"9일 국토교통부 실거래가 공개시스템 등에 따르면, 광교중흥S클래스 전용 109㎡는 지난달 17일 17억5000만원에 매매됐다. 이보다 한 달 전인 10월 18일에는 26억2000만원에 직거래된 적이 있다.\n",
        "\n",
        "이 아파트 전용 109㎡는 지난해 6월 22일 27억원에 거래돼 최고점을 찍었다. 이후 10개월가량 매매가 없다가, 올해 4월과 5월 각각 20억2000만~20억3000만원에 손바뀜됐다. 그러다 다섯 달만에 돌연 6억 오른 가격에 직거래됐다가, 다시 가격이 크게 떨어진 것이다. 전용 85㎡도 지난해 7~11월에는 18억원에 매매가 이뤄졌으나, 이후 꾸준히 하락세를 보였다. 지난달에는 고점 대비 6억3000만원 빠진 11억7000만원에 거래됐다.\n",
        "\n",
        "10억원 가까이 빠진 가격에 매매가 이뤄졌다는 소식이 알려지자 부동산 커뮤니티들이 달아올랐다. 한 네티즌은 “서울도 아니고 40평대가 20억원을 넘는 것은 말이 안 되는 거품이었다”고 했다. 다른 네티즌은 “(10억원이 내렸지만) 아직도 비싸다”고 했다. “분양가를 감안하면 (기존 최고가보다) 10억원가량 싸게 팔아도 남는 장사”라는 이들도 있었다. 반면 입주민으로 보이는 네티즌들은 “전국적으로 하락장이고, 1주택 실거주라 (집값이) 내려도 부담이 없다. 마음 편하게 사는 게 최고”라고 했다. “급등했으니 급락하는 것도 당연하다”며 동조하는 이들도 있었다.\n",
        "\n",
        "한국부동산원에 따르면, 광교신도시가 포함된 수원 영통구의 올해 집값 누적하락률은 12.53%에 달한다. 광교 뿐만 아니라 경기 남부의 다른 신도시인 동탄 역시 집값이 하락세를 보이고 있다. 동탄 더샵레이크 에듀타운 전용 84㎡는 지난달 15일 7억원에 거래됐다. 최고가인 12억1700만원에 비교하면 5억원 이상 낮다. 힐스테이트동탄 84㎡는 지난달 18일 5억8000만원에 매매가 이뤄졌다. 이전 최고가는 9억6500만원이었다. 동탄이 포함된 화성의 올해 집값 누적 하락률은 10.92%였다.\n",
        "\n",
        "아파트 시장의 침체기는 이어지고 있다. 8일 한국부동산원이 발표한 주간 아파트 가격동향(5일 기준)을 보면, 이번주 전국 아파트 매매가격은 전주 대비 0.59% 떨어졌다. 2012년 한국부동산원 시세 조사 이래 가장 큰폭의 하락세다. 서울은 0.59%, 경기는 0.78% 내렸다. 주택가격 추가 하락 우려와 금리인상에 대한 부담으로 매수 문의가 거의 없는 상황이 이어지는 가운데, 일부 급매물이 소화되면서 가격이 전반적으로 떨어지고 있다는 진단이다.\"\"\"\n",
        "model_input = tokenizer(\n",
        "            sentence_text,\n",
        "            return_tensors='np',\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "input_ids = model_input[\"input_ids\"]\n",
        "attention_mask = model_input[\"attention_mask\"]\n",
        "\n",
        "tokens_tensor = torch.tensor(input_ids)\n",
        "segments_tensors = torch.tensor(attention_mask)"
      ],
      "metadata": {
        "id": "FHr7ivWQaFqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = torch.jit.load(\"traced_bart.pt\")\n",
        "loaded_model.eval()\n",
        "\n",
        "dummy_input = [tokens_tensor, segments_tensors]\n",
        "\n",
        "result = loaded_model(*dummy_input)"
      ],
      "metadata": {
        "id": "ayq35UYSZxXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir5VFrYOaNZC",
        "outputId": "29966318-f5b0-4185-95b4-ae2f5ce6757d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 93])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(result.squeeze().tolist(), skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BS99t1Yxg4cR",
        "outputId": "78c49a0c-4b72-452c-ae21-0d4a909eba2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'국토교통부 실거래가 공개시스템 등에 따르면, 광교중흥S클래스 전용 109m2는 지난달 17일 17억5000만원에 매매됐다. 이보다 한 달 전인 10월 18일에는 26억2000만원에 직거래된 적이 있다. 이후 10개월가량 매매가 없다가, 올해 4월과 5월 각각 20억2000만~20억3000만원에 손바뀜됐다가, 다시 가격이 크게 떨어진 것이다. 전용 85m2도'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config.pbtxt\n",
        "\n",
        "```text\n",
        "name: \"kobart_summarization\"\n",
        "platform: \"pytorch_libtorch\"\n",
        "input [\n",
        " {\n",
        "    name: \"input__0\"\n",
        "    data_type: TYPE_INT32\n",
        "    dims: [1, 512]\n",
        "  } ,\n",
        "{\n",
        "    name: \"input__1\"\n",
        "    data_type: TYPE_INT32\n",
        "    dims: [1, 512]\n",
        "  }\n",
        "]\n",
        "output {\n",
        "    name: \"output__0\"\n",
        "    data_type: TYPE_FP32\n",
        "    dims: [1, 3]\n",
        "  }\n",
        "```"
      ],
      "metadata": {
        "id": "I7Q6KYO2mV2Z"
      }
    }
  ]
}